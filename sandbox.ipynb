{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pytesseract\n",
    "from flask import Flask, request, jsonify\n",
    "from pdf2image import convert_from_bytes\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Meshal\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\n",
    "def extract_text_from_pdf(pdf):\n",
    "    \"\"\"\n",
    "    A function to convert multi-page pdf to images\n",
    "    @misc{PythonTe58:online,\n",
    "    author = {},\n",
    "    title = {Python Tesseract PDF & OCR Example - Data Analytics},\n",
    "    howpublished = {url{https://vitalflux.com/python-tesseract-pdf-ocr-example/}},\n",
    "    month = {},\n",
    "    year = {},\n",
    "    note = {(Accessed on 06/01/2023)}\n",
    "    }\n",
    "    \"\"\"\n",
    "    pages = convert_from_bytes(pdf)\n",
    "    text_data = ''\n",
    "    for page in pages:\n",
    "        text = pytesseract.image_to_string(page)\n",
    "        text_data += text + '\\n'\n",
    "    return text_data\n",
    "\n",
    "\n",
    "def extract(pdf_file):\n",
    "    \"\"\"\n",
    "    A function that extracts text from PDF documents.\n",
    "    \"\"\"\n",
    "    pdf_data = pdf_file.read()\n",
    "    pdf_text = extract_text_from_pdf(pdf_data)\n",
    "    print(pdf_text)\n",
    "    # return jsonify({'data': pdf_text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meshal Alamr\n",
      "\n",
      "An adaptable optimistic team player that is enthusiastic about Artificial Intelligence, Data Science, Automation, Speech Processing, Large Language\n",
      "Models, and Computer Vision. A person with a versatile skill-set, a lot of integrity, and a willingness to go the extra mile to learn something new.\n",
      "\n",
      "§M% AlamrMeshal@gmail.com | +966 55 375 2825\n",
      "\n",
      "lin] linkedin.com/in/MeshalAlamr\n",
      "\n",
      "©) github.com/MeshalAlamr\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Artificial Intelligence Engineer / Researcher\n",
      "\n",
      "Confidential Government\n",
      "\n",
      "03/2022 - Present\n",
      "Applied Research Department (ARD)\n",
      "\n",
      "Riyadh, Saudi Arabia\n",
      "\n",
      "Job Description\n",
      "\n",
      "e Worked on cutting-edge deep learning projects that mainly involve\n",
      "Speech Processing, Large Language Models, and some Computer\n",
      "Vision.\n",
      "\n",
      "e Responsibilities included being a Technical Lead For overseeing\n",
      "projects' development and implementation.\n",
      "\n",
      "e Utilized PyTorch and Hugging Face to train models.\n",
      "\n",
      "e Utilized Docker and Docker Compose to containerize and manage\n",
      "Al applications, ensuring consistent and reproducible deployments\n",
      "of multi-container environments.\n",
      "\n",
      "@ Created web applications using Flask For backend development\n",
      "and integrated front-end interfaces with Tailwind CSS.\n",
      "\n",
      "® Utilized collaboration tools such as GitHub, Slack, Notion, and\n",
      "Mendeley for streamlined communication, organization, and\n",
      "knowledge sharing.\n",
      "\n",
      "© Actively participated in continuous learning initiatives, attending\n",
      "\n",
      "conferences, and workshops to stay abreast of the latest\n",
      "advancements in the Field.\n",
      "\n",
      "Data Scientist Trainee\n",
      "\n",
      "SDAIA Academy - SDAIA T5 Bootcamp\n",
      "09/2021 - 11/2021\n",
      "\n",
      "Job Description\n",
      "\n",
      "e Intensive modules and practical projects about data science,\n",
      "\n",
      "specifically on exploratory data analysis, web scraping, machine\n",
      "learning and deep learning.\n",
      "\n",
      "Riyadh, Saudi Arabia\n",
      "\n",
      "Electrical Engineering Intern\n",
      "\n",
      "Saudi Aramco\n",
      "\n",
      "06/2021 - 08/2021 Riyadh, Saudi Arabia\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "Bachelor's of Electrical Engineering - Robotics\n",
      "and Autonomous Systems\n",
      "\n",
      "King Fahd University of Petroleum and Minerals\n",
      "(KFUPM)\n",
      "09/2016 - 09/2021\n",
      "Modules\n",
      "\n",
      "e Undergraduate Research\n",
      "[Computer Vision Project].\n",
      "\n",
      "3.72 / 4.00 Honors Class\n",
      "\n",
      "e Artificial Intelligence &\n",
      "Machine Learning for\n",
      "Robotics.\n",
      "\n",
      "Student Exchange Program\n",
      "Georgia Institute of Technology\n",
      "08/2019 - 12/2019\n",
      "\n",
      "Description\n",
      "\n",
      "e Exchange Student in the\n",
      "United States for the Fall\n",
      "Semester of 2019.\n",
      "\n",
      "SKILLS\n",
      "\n",
      "C= £3 EIS\n",
      "\n",
      "ACHIEVEMENTS\n",
      "\n",
      "Honors - Deanship of Student Affairs\n",
      "KFUPM Honors , 2017 - 2020\n",
      "\n",
      "Award for Excellence in English Writing\n",
      "KFUPM (Top 0.79%) , 2017 ~ Top (0.83%), 2018\n",
      "\n",
      "PROJECTS\n",
      "\n",
      "Thmanya Written (05/2023)\n",
      "\n",
      "© Aninnovative Al-powered service that leverages audio transcription to\n",
      "Facilitate seamless access and interactions with podcast content.\n",
      "\n",
      "Speech Emotion Recognition - SDAIA T5 Bootcamp (11/2021)\n",
      "\n",
      "® Detecting emotion from speech signals using CNN to help school\n",
      "consultancies intervene in case a child shows early signs of mental illness.\n",
      "\n",
      "Flight Price Prediction - SDAIA T5 Bootcamp (10/2021)\n",
      "\n",
      "® Predicting Flight ticket prices using a random forest regression model\n",
      "based on scraped data from Kayak. An Android mobile app was also\n",
      "developed.\n",
      "\n",
      "Spatial Temporal Graph Convolutional Networks for the\n",
      "Recognition of Quick Human Actions - KFUPM\n",
      "(01/2021 - 05/2021)\n",
      "\n",
      "® Developed a spatial-temporal graph convolutional neural network (ST-GCN)\n",
      "based deep learning model using PyTorch to recognize quick human actions\n",
      "and wrote a research paper about the project.\n",
      "\n",
      "CERTIFICATES / COURSES\n",
      "\n",
      "ChatGPT Prompt Engineering For Developers\n",
      "OpenAl, DeepLearning.Al, 2023\n",
      "\n",
      "Learning How to Learn\n",
      "Coursera, 2022\n",
      "\n",
      "Research Skills\n",
      "KFUPM, 2020\n",
      "\n",
      "TOEFL iBT (100/120)\n",
      "ETS, 2018\n",
      "\n",
      "LANGUAGES\n",
      "\n",
      "Arabic English\n",
      "Native or Bilingual Proficiency Full Professional Proficiency\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract(open('Meshal_Alamr_CV.pdf', 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Meshal\\Anaconda3\\envs\\ml-task\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "hf_name = 'pszemraj/led-base-book-summary'\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    hf_name,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    # device = -1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In this paper, we examine the literature dealing with the \"adverse effect\" of information-overload, i.e., information fatigue syndrome. The authors focus on three main areas: technology as an aid to overcome information overload; increased informationliteracy; and increasing information literacy. In particular, they discuss information overloads in business enterprises where people are constantly being bombarded by a large number of unstructured, unsolicited, or incomplete information. This information overload problem is particularly prevalent in the age of information technology. Various studies have examined the effects of information overload on individuals' ability to effectively deal with complex information. According to one study, communication overload occurs when too many messages compete for our time but do not provide sufficient information to adequately process them. Other studies suggest that it can be avoided if managers learn to use concepts rather than relying on details and data. An intelligent agent such as an information specialist may reduce information load because there is more information available at a faster rate, thereby improving efficiency"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wall_of_text = \"Meshal Alamr An adaptable optimistic team player that is enthusiastic about Artificial Intelligence, Data Science, Automation, Speech Processing, Large Language Models, and Computer Vision. A person with a versatile skill-set, a lot of integrity, and a willingness to go the extra mile to learn something new. §M% AlamrMeshal@gmail.com | +966 55 375 2825 lin] linkedin.com/in/MeshalAlamr ©) github.com/MeshalAlamr WORK EXPERIENCE Artificial Intelligence Engineer / Researcher Confidential Government 03/2022 - Present Applied Research Department (ARD) Riyadh, Saudi Arabia Job Description e Worked on cutting-edge deep learning projects that mainly involve Speech Processing, Large Language Models, and some Computer Vision. e Responsibilities included being a Technical Lead For overseeing projects' development and implementation. e Utilized PyTorch and Hugging Face to train models. e Utilized Docker and Docker Compose to containerize and manage Al applications, ensuring consistent and reproducible deployments of multi-container environments. @ Created web applications using Flask For backend development and integrated front-end interfaces with Tailwind CSS. ® Utilized collaboration tools such as GitHub, Slack, Notion, and Mendeley for streamlined communication, organization, and knowledge sharing. © Actively participated in continuous learning initiatives, attending conferences, and workshops to stay abreast of the latest advancements in the Field. Data Scientist Trainee SDAIA Academy - SDAIA T5 Bootcamp 09/2021 - 11/2021 Job Description e Intensive modules and practical projects about data science, specifically on exploratory data analysis, web scraping, machine learning and deep learning. Riyadh, Saudi Arabia Electrical Engineering Intern Saudi Aramco 06/2021 - 08/2021 Riyadh, Saudi Arabia EDUCATION Bachelor's of Electrical Engineering - Robotics and Autonomous Systems King Fahd University of Petroleum and Minerals (KFUPM) 09/2016 - 09/2021 Modules e Undergraduate Research [Computer Vision Project]. 3.72 / 4.00 Honors Class e Artificial Intelligence & Machine Learning for Robotics. Student Exchange Program Georgia Institute of Technology 08/2019 - 12/2019 Description e Exchange Student in the United States for the Fall Semester of 2019. SKILLS C= £3 EIS ACHIEVEMENTS Honors - Deanship of Student Affairs KFUPM Honors , 2017 - 2020 Award for Excellence in English Writing KFUPM (Top 0.79%) , 2017 ~ Top (0.83%), 2018 PROJECTS Thmanya Written (05/2023) © Aninnovative Al-powered service that leverages audio transcription to Facilitate seamless access and interactions with podcast content. Speech Emotion Recognition - SDAIA T5 Bootcamp (11/2021) ® Detecting emotion from speech signals using CNN to help school consultancies intervene in case a child shows early signs of mental illness. Flight Price Prediction - SDAIA T5 Bootcamp (10/2021) ® Predicting Flight ticket prices using a random forest regression model based on scraped data from Kayak. An Android mobile app was also developed. Spatial Temporal Graph Convolutional Networks for the Recognition of Quick Human Actions - KFUPM (01/2021 - 05/2021) ® Developed a spatial-temporal graph convolutional neural network (ST-GCN) based deep learning model using PyTorch to recognize quick human actions and wrote a research paper about the project. CERTIFICATES / COURSES ChatGPT Prompt Engineering For Developers OpenAl, DeepLearning.Al, 2023 Learning How to Learn Coursera, 2022 Research Skills KFUPM, 2020 TOEFL iBT (100/120) ETS, 2018 LANGUAGES Arabic English Native or Bilingual Proficiency Full Professional Proficiency\"\n",
    "text = '''The problem of information overload is widely recognised today. Living in an “information society”, we are bombarded with information whether or not we actively seek it. We are all affected by the increasing number of sources from which information emanates. Who does not receive unwanted information through the letter box almost daily? Growing numbers of television channels provide more viewing choice and result in burgeoning programme guides to be ploughed through in the often elusive search for an interesting programme to watch amongst the escalating dross churned out. Newspapers, radio and television often disseminate the same news items with such an intensity of coverage that one can soon develop a perception of information overload but here one can choose to ignore that information. In the workplace, however, information is seen as the key to success for organisations and many people have to deal with an overwhelming amount of information from many sources as part of their job. People cannot afford to ignore information in the workplace. “Professional and personal survival in modern society clearly depends on our ability to take on board vast amounts of new information. Yet that information is growing at an exponential rate” (Lewis, 1996).\n",
    "\n",
    "“The technological developments of the last 50 years have made more information more available to more people than at any other time in human history” (Feather, 1998). While there are obvious benefits from easier access to information, research has found that information overload can lead to stress, loss of job satisfaction and physical ill health (Lewis, 1996).\n",
    "\n",
    "The machines we have invented to produce, manipulate and disseminate information generate information much faster than we can process it. It is apparent that an abundance of information, instead of better enabling a person to do their job, threatens to engulf and diminish his or her control over the situation. It is now widely recognised that stress can be experienced from a feeling of lack of control. We can unwittingly allow information technology to become the driver instead of harnessing it as tool to enhance rather than diminish our lives. The problem of information overload is obviously not going to recede and solutions need to be found to enable people to reduce the amount of information overload they experience.\n",
    "\n",
    "This paper reviews the literature on the subject of information overload, with particular reference to business organisations, in order to provide an overview of the growth of the problem and the current situation. References to the problem of information overload are scattered throughout the literature and this review is, therefore, necessarily selective while aiming to present a balanced overview highlighting significant points garnered from extensive reading of the literature. Various solutions that have been put forward in the literature are briefly described and suggestions for further research are made.\"'''\n",
    "\n",
    "result = summarizer(\n",
    "           text,\n",
    "           min_length=8, \n",
    "           max_length=1000,\n",
    "           no_repeat_ngram_size=3, \n",
    "           encoder_no_repeat_ngram_size=3,\n",
    "           repetition_penalty=3.5,\n",
    "           num_beams=4,\n",
    "           do_sample=False,\n",
    "           early_stopping=True,\n",
    "    )\n",
    "# result = summarizer(\n",
    "#             wall_of_text,\n",
    "#             max_length=100,\n",
    "#             min_length=5,\n",
    "#             no_repeat_ngram_size=3, \n",
    "#             encoder_no_repeat_ngram_size=3,\n",
    "#             repetition_penalty=3.5,\n",
    "#             do_sample=False,\n",
    "#             num_beams=4,\n",
    "#             early_stopping=True,\n",
    "#             )\n",
    "display(Markdown(result[0]['summary_text']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['queries', 'resources']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all collections\n",
    "db = client['mltask']\n",
    "for collection in db.list_collection_names():\n",
    "    db.drop_collection(collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.mltask  # Replace \"mltask\" with the name of your database\n",
    "\n",
    "for collection in ['resources', 'queries']:\n",
    "    if collection not in db.list_collection_names():\n",
    "        db.create_collection(collection)\n",
    "\n",
    "resources = db.resources\n",
    "quries = db.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x180bba33c40>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resources.insert_many([\n",
    "    {'type': 'article',\n",
    "    'name': 'The problem of information overload in business organisations: a review of the literature',\n",
    "    'author': 'Angela Edmunds, Anne Morris',\n",
    "    'year': 2000,\n",
    "    'publisher': 'International Journal of Information Management',\n",
    "    'summary': 'This paper reviews the literature on the problem of information overload, with particular reference to business organisations. A theme stressed in the literature is the paradoxical situation that it is often difficult to obtain useful, relevant infor-\\nmation when it is needed. An emphasis is placed on technology as a tool and not the driver.',\n",
    "    'sentiment': 'positive'\n",
    "    \n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all documents in the collection\n",
    "collection = db.resources\n",
    "all_documents = collection.find()\n",
    "for document in all_documents:\n",
    "    print(document)\n",
    "\n",
    "# Find documents that match a specific condition\n",
    "query = {\"author\": \"Angela Edmunds, Anne Morris\"}\n",
    "matching_documents = collection.find(query)\n",
    "for document in matching_documents:\n",
    "    print(document)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'LABEL_2', 'score': 0.9050958156585693}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "sentiment_pipeline(text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict[sentiment_pipeline(text)[0]['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "1) positive 0.9051\n",
      "2) neutral 0.0936\n",
      "3) negative 0.0013\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=['negative', 'neutral', 'positive']\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "\n",
    "text = \"Good night 😊\"\n",
    "text = \"\"\"\n",
    "This is a very exciting chapter in the life of a talented artificial intelligence engineer named Meshal al- Alamr. He will be studying Artificial Intelligence and Machine Learning at the prestigious Saudia Academy, Riyadh, during the academic year that begins in 2021. The position description describes his extensive training in Python, Docker, and C# containerization. He also has a strong commitment to experimentation and continuous learning. In addition, he plans to work as a technical lead for overseeing projects' development & implementation. At present, he is completing \"continuous learning initiatives,\" attending Conferences and workshops, and contributing to the project's progress. His resume includes: \"A bachelor's degree in electrical engineering - Robotics and autonomous systems; Ph.D.; Doctorate in Electrical Engineering; Bachelor's of Mechanical Engineering; Civil Engineering; KUPM; Behavioural Management; Sociology; Arts and Crafts; Geography; Education; Translation; Achieving greater understanding of the world through practical applications\n",
    "\"\"\"\n",
    "# text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "print(labels[ranking[0]])\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from scipy.special import expit\n",
    "\n",
    "    \n",
    "MODEL = f\"cardiffnlp/tweet-topic-21-multi\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "class_mapping = model.config.id2label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business & Entrepreneurs\n",
      "Science & Technology\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "This paper reviews the literature on the problem of information overload, with particular reference to business organisations. A theme stressed in the literature is the paradoxical situation that it is often difficult to obtain useful, relevant infor-\n",
    "mation when it is needed. An emphasis is placed on technology as a tool and not the driver.\"\"\"\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "output = model(**tokens)\n",
    "\n",
    "scores = expit(output[0][0].detach().numpy())\n",
    "predictions = (scores >= 0.4) * 1\n",
    "\n",
    "# Map to classes\n",
    "for i in range(len(predictions)):\n",
    "  if predictions[i]:\n",
    "    pred = class_mapping[i].replace('_', ' ').title()\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pytesseract\n",
    "from flask import Flask, request, jsonify\n",
    "from pdf2image import convert_from_bytes\n",
    "import base64\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from flask_pymongo import PyMongo\n",
    "from scipy.special import expit\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "summarizer_hf = \"pszemraj/led-base-book-summary\"\n",
    "\n",
    "summarizer_pipeline = pipeline(\n",
    "    \"summarization\",\n",
    "    summarizer_hf,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "topic_hf = \"cardiffnlp/tweet-topic-21-multi\"\n",
    "topic_tokenizer = AutoTokenizer.from_pretrained(topic_hf)\n",
    "topic_model = AutoModelForSequenceClassification.from_pretrained(topic_hf)\n",
    "topic_class_mapping = topic_model.config.id2label\n",
    "\n",
    "\n",
    "sentiment_hf = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "sentiment_labels = {\n",
    "    \"LABEL_0\": 'negative',\n",
    "    \"LABEL_1\": 'neutral',\n",
    "    \"LABEL_2\": 'positive'\n",
    "}\n",
    "\n",
    "sentiment_pipeline = pipeline(\n",
    "    task='sentiment-analysis',\n",
    "    model=sentiment_hf,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = AutoModelForSequenceClassification.from_pretrained('models/topic_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_tokenizer.save_pretrained('models/topic_model')\n",
    "topic_model.save_pretrained('models/topic_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_pipeline.save_pretrained('summarizer')\n",
    "topic_model.save_pretrained('topic_model')\n",
    "sentiment_pipeline.save_pretrained('sentiment_pipeline')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "# User's search query\n",
    "search_query = \"information overload in business organizations\"\n",
    "\n",
    "# Fetch all documents from the collection\n",
    "documents = resources.find()\n",
    "\n",
    "# Initialize empty lists to store summaries and document IDs\n",
    "summaries = []\n",
    "document_ids = []\n",
    "\n",
    "# Iterate over the documents and extract summaries\n",
    "for doc in documents:\n",
    "    if 'summary' in doc:\n",
    "        summaries.append(doc['summary'])\n",
    "        document_ids.append(doc['_id'])\n",
    "\n",
    "# Add the search query to the list of summaries\n",
    "summaries.append(search_query)\n",
    "\n",
    "embeddings = model.encode(summaries)\n",
    "\n",
    "# Calculate cosine similarity between the search query and summaries\n",
    "query_embedding = embeddings[-1]\n",
    "cosine_similarities = cosine_similarity(query_embedding.reshape(1, -1), embeddings[:-1])\n",
    "\n",
    "# Get the indices of the most similar documents\n",
    "most_similar_indices = cosine_similarities.argsort()[0][-5:][::-1]\n",
    "\n",
    "# Retrieve the most similar documents from the database\n",
    "most_similar_documents = collection.find({'_id': {'$in': [document_ids[idx] for idx in most_similar_indices]}})\n",
    "\n",
    "# Print the most similar documents\n",
    "for doc in most_similar_documents:\n",
    "    print(\"Document Name:\", doc['name'])\n",
    "    print(\"Author:\", doc['author'])\n",
    "    print(\"Summary:\", doc['summary'])\n",
    "    print(\"Sentiment:\", doc['sentiment'])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"information overload in business organizations\"\n",
    "summary = \"This paper reviews the literature on the problem of information overload, with particular reference to business organisations. A theme stressed in the literature is the paradoxical situation that it is often difficult to obtain useful, relevant infor- mation when it is needed. An emphasis is placed on technology as a tool and not the driver.\"\n",
    "topic = \"None\"\n",
    "sentiment = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It sounds like you're interested in learning about managing information overload in business organizations. I've got just the resource for you! Our paper reviews the latest literature on this topic and emphasizes the role of technology as a tool, rather than a driver. We understand how daunting this issue can be, but don't worry - our team of experts have researched this extensively to provide the most insightful and relevant information for your needs. Let us help you make navigating information overload in business organizations a breeze!\n"
     ]
    }
   ],
   "source": [
    "prompt_template = f\"\"\"You are a smart helpful librarian that will assist users in finding resources. \n",
    "Given a user's query, and the most relevant resource's summary, topic, and sentiment, provide a response to the user's query.\n",
    "Be enthusiastic in your response and relate to the user's query.\n",
    "\n",
    "User Query: {query}\n",
    "Resource Summary: {summary}\n",
    "Resource Topic: {topic}\n",
    "Resource Sentiment: {sentiment}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "ChatML = [\n",
    "    {'role' : 'system', 'content' : 'You are a smart helpful librarian that will assist users in finding resources.'},\n",
    "    {'role' : 'user', 'content' : prompt_template}\n",
    "     ]\n",
    "completion = openai.ChatCompletion.create(model='gpt-3.5-turbo', messages=ChatML)\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
